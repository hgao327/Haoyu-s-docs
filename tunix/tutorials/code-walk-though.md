# Google Tunix æºç åˆ†æ

### 1. `tunix/rl/grpo/grpo_learner.py`

####  `train()`ï¼šä¸»è®­ç»ƒå¾ªç¯

- æ•´ä¸ª GRPO çš„è®­ç»ƒæµç¨‹å…¥å£ã€‚
- å¯¹åº” GRPO è®ºæ–‡ä¸­çš„ outer loopï¼ˆæ¯è½® iterationï¼‰ã€‚
- æ ¸å¿ƒåŒ…æ‹¬ï¼š
  1. **prepare_dataset**ï¼šé‡‡æ ·å¤šä¸ª completionsï¼Œæ‰“åˆ†ï¼Œè®¡ç®—ä¼˜åŠ¿ã€‚
  2. **trainer.train**ï¼šè¿›è¡Œ Î¼ æ¬¡å‚æ•°æ›´æ–°ï¼ˆå†…å±‚ RL æ›´æ–°ï¼‰ã€‚
  3. **sync_sampler_weights**ï¼šå°† trainer çš„æ¨¡å‹å‚æ•°åŒæ­¥ç»™ samplerã€‚

------

#### ğŸŸ¦ `prepare_dataset()`ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®

- æ¥æ”¶ prompt datasetï¼Œç”Ÿæˆè®­ç»ƒæ•°æ® `TrainExample`ã€‚
- æ¯æ¡ prompt é‡‡æ · G ä¸ª completions â†’ ç”¨ reward model æ‰“åˆ†ã€‚
- è°ƒç”¨ `_generate_and_compute_advantage` æ¥å¤„ç†é‡‡æ ·å’Œ advantage è®¡ç®—ã€‚

------

#### ğŸŸ¨ `_generate_and_compute_advantage()`ï¼šç”Ÿæˆ + æ‰“åˆ† + Advantage è®¡ç®—

- è°ƒç”¨ `rollout_worker.generate()` ç”Ÿæˆå¤šä¸ªå›å¤ã€‚
- ä½¿ç”¨ `reward_fn(prompts, completions)` å¯¹æ¯ä¸ªå›å¤æ‰“åˆ†ã€‚
- è°ƒç”¨ `compute_advantages()` è¿›è¡Œå½’ä¸€åŒ–ã€‚

------

#### ğŸŸ© `compute_advantages()`ï¼š

```python
(rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)
```

- å®ç°è®ºæ–‡ä¸­çš„ group relative advantageã€‚
- å¯¹æ¯ç»„ G ä¸ªå›ç­”ï¼Œè¿›è¡Œç»„å†…æ ‡å‡†åŒ–ã€‚

------

#### ğŸŸ§ `_compute_rewards()`ï¼š

- æ”¯æŒå¤šä¸ª reward functionï¼Œå¯¹æ¯ä¸ªå›å¤è°ƒç”¨ `reward_fn(prompts, completions)`ã€‚
- è¿”å› `[num_prompts * G]` é•¿åº¦çš„å¥–åŠ±åˆ†æ•°ã€‚

------

#### ğŸŸ¥ `grpo_loss_fn()`ï¼š

- é‡‡ç”¨ PPO-style clipped lossï¼š

```python
- min(r_t * A, clip(r_t, 1 - Îµ, 1 + Îµ) * A)
```

- åŒæ—¶åŠ å…¥ KL çº¦æŸé¡¹ï¼š

```python
+ Î² * KL(trained || reference)
```

- è¿™é‡Œä¸ä½¿ç”¨ value functionï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨ä¼˜åŠ¿ï¼ˆAï¼‰æ¥è‡ª rewardã€‚

------

#### ğŸ” `trainer.train()`ï¼š

- å®é™…æ‰§è¡Œæ¢¯åº¦åå‘ä¼ æ’­ï¼Œå‚æ•°æ›´æ–°ã€‚
- ä¼šæ‰§è¡Œ Î¼ æ¬¡ï¼ˆå³ `GrpoConfig.num_iterations`ï¼‰æ¯æ¬¡ä½¿ç”¨ç›¸åŒçš„æ ·æœ¬ã€‚

------

#### ğŸ”„ `sync_sampler_weights()`ï¼š

- å°† `trainer` æ›´æ–°åçš„å‚æ•°åŒæ­¥ç»™ `rollout_worker`ã€‚
- å¦‚æœæ¨¡å‹ä½¿ç”¨äº† LoRAï¼Œä¼šä»…åŒæ­¥ LoRA æƒé‡ã€‚









### `2. tunix/rl/inference/inference_worker.py`

è¿™æ®µä»£ç ä½äº `tunix.rl` æ¨¡å—ï¼Œå®šä¹‰äº†ä¸€ä¸ªåä¸º `InferenceWorker` çš„ç±»ï¼Œç”¨äºæ‰˜ç®¡åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æ¨ç†é˜¶æ®µä¼šç”¨åˆ°çš„æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š

- `critic`ï¼šç”¨æ¥ä¼°ç®—çŠ¶æ€-åŠ¨ä½œå€¼ï¼ˆæœªå®ç°ï¼‰
- `reference`ï¼šå‚è€ƒæ¨¡å‹ï¼Œç”¨äºè®¡ç®— KL æ•£åº¦
- `reward`ï¼šå¥–åŠ±æ¨¡å‹ï¼Œç”¨æ¥å¯¹ç”Ÿæˆçš„å›ç­”è¯„åˆ†ï¼ˆæœªå®ç°ï¼‰

------

#### å¼•å…¥çš„æ¨¡å—

```python
from flax import nnx
import jax
from tunix.rl import common
```

- `nnx` æ˜¯ Flax ä¸­çš„æ¨¡å—æ„å»ºç³»ç»Ÿ
- `jax` æä¾›æ•°ç»„å’Œè®¾å¤‡åŠ é€Ÿ
- `common` æ¨¡å—ä¸­åŒ…å«ä¸€äº›å¸¸ç”¨å‡½æ•°ï¼Œæ¯”å¦‚è®¡ç®— logp çš„æ–¹æ³•

------

#### ç±»å®šä¹‰ï¼š`InferenceWorker`

```python
class InferenceWorker:
  """Inference worker hosting critic, reference and reward models."""
```

è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„æ¨ç†å·¥ä½œç±»ï¼Œè´Ÿè´£â€œæ‰˜ç®¡â€ä¸€äº› RL ç›¸å…³çš„æ¨¡å‹ã€‚

------

#### æ„é€ å‡½æ•°

```python
def __init__(self, models: dict[str, nnx.Module]):
```

æ¥å—ä¸€ä¸ª `dict` ç±»å‹çš„æ¨¡å‹å®¹å™¨ã€‚å®ƒæ£€æŸ¥ï¼š

```python
for k in models.keys():
  if k not in ["critic", "reference", "reward"]:
    raise ValueError(...)
```

ç¡®ä¿åªåŒ…å«åˆæ³•çš„æ¨¡å‹è§’è‰²ã€‚

æ¥ç€å®ƒå°† `models` å­˜å…¥å®ä¾‹å˜é‡ï¼š

```python
self._models = models
```

------

#### æ–¹æ³•ï¼š`get_ref_per_token_logps`

```python
def get_ref_per_token_logps(self, prompt_tokens, completion_tokens, pad_id, eos_id):
```

æ­¤å‡½æ•°ä½¿ç”¨å‚è€ƒæ¨¡å‹ï¼ˆ`reference`ï¼‰æ¥è®¡ç®—ï¼š

- è¾“å…¥ä¸º prompt å’Œç”Ÿæˆçš„ completionï¼ˆä¸¤è€…éƒ½ä¸º token æ•°ç»„ï¼‰
- è°ƒç”¨ `common.compute_per_token_logps` å‡½æ•°è¿›è¡Œå¤„ç†

```python
return common.compute_per_token_logps(...)
```

è¿™ä¸ªå‡½æ•°æœ€ç»ˆè¿”å›ä¸€ä¸ª `jax.Array`ï¼Œè¡¨ç¤ºå‚è€ƒæ¨¡å‹åœ¨æ¯ä¸ªç”Ÿæˆ token ä¸Šçš„ log æ¦‚ç‡ï¼Œå¸¸ç”¨äºè®¡ç®— RL ä¸­çš„ KL æ•£åº¦çº¦æŸé¡¹ã€‚

------

#### æ–¹æ³•ï¼š`compute_rewards`

```python
def compute_rewards(self):
  raise NotImplementedError()
```

ä¿ç•™æ¥å£æœªå®ç°ï¼Œæœªæ¥ç”¨äºä½¿ç”¨ reward æ¨¡å‹æ‰“åˆ†ã€‚

------

#### æ–¹æ³•ï¼š`compute_values`

```python
def compute_values(self):
  raise NotImplementedError()
```



è¿™ä¸ªæ¨¡å‹**ä¸æ˜¯ç”¨æ¥ç”Ÿæˆæ–‡æœ¬çš„**ï¼Œè€Œæ˜¯ç”¨æ¥å¯¹ç”Ÿæˆç»“æœè¿›è¡Œ**è¯„åˆ†ï¼ˆrewardï¼‰**ã€**å‚è€ƒå¯¹æ¯”ï¼ˆreference KLï¼‰å’Œä¼°å€¼ï¼ˆcriticï¼‰**ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„è¯„ä¼°é˜¶æ®µã€‚ç”Ÿæˆæ–‡æœ¬æ˜¯ç”± `actor` æ¨¡å‹å®Œæˆçš„ã€‚









### 3. `tunix/rl/queue/data_queue.py`

------

#### ğŸ‘‡ åœ¨ `GrpoLearner.train()` ä¸­ä¼šçœ‹åˆ°è¿™æ ·ä¸€æ®µä»£ç ï¼š

```python
train_data_queue = queue_lib.SimpleDataQueue(maxsize=self.grad_acc_steps + 2)
```

ç„¶åå¼‚æ­¥è°ƒç”¨ï¼š

```python
self.executor.submit(
    self.prepare_dataset,
    ...,
    data_queue=train_data_queue,
)
```

å†åœ¨ä¸»çº¿ç¨‹ä¸­æ¶ˆè´¹ï¼š

```python
while True:
  curr_train_ds = train_data_queue.get(block=True)
  if curr_train_ds is None:
    break
  self.rl_cluster.update_actor(curr_train_ds, ...)
```

------

#### ğŸ”„ è¿™èƒŒåçš„æµç¨‹ç»“åˆ `SimpleDataQueue` æ˜¯è¿™æ ·çš„ï¼š

| æ“ä½œ         | æ–¹æ³•                   | ä½œç”¨                                           |
| ------------ | ---------------------- | ---------------------------------------------- |
| æ•°æ®å‡†å¤‡çº¿ç¨‹ | `data_queue.put(...)`  | æŠŠ `TrainExample` æˆ– `RepeatIterable` æ”¾å…¥é˜Ÿåˆ— |
| ä¸»è®­ç»ƒçº¿ç¨‹   | `data_queue.get()`     | ä»é˜Ÿåˆ—ä¸­å–å‡ºæ•°æ®å¹¶è¿›è¡Œè®­ç»ƒ                     |
| è®­ç»ƒå®Œæˆ     | `data_queue.put(None)` | å‘é€â€œç»“æŸä¿¡å·â€                                 |
| æ¸…ç†èµ„æº     | `data_queue.close()`   | æ¸…ç©ºé˜Ÿåˆ—ä¸­æ®‹ç•™çš„æ•°æ®                           |

------

#### ğŸ§  å…³é”®ç‰¹æ€§

- `SimpleDataQueue` æ˜¯ `AbstractDataQueue` çš„å…·ä½“å®ç°ï¼Œåº•å±‚ä½¿ç”¨äº† Python æ ‡å‡†åº“çš„ `queue.Queue`
- ä½¿ç”¨äº†æ³›å‹ `_T`ï¼Œå¯ä»¥ä¼ é€’ä»»æ„ç±»å‹æ•°æ®ï¼ˆå¦‚ `TrainExample`, `List[TrainExample]`ï¼‰
- `close()` æ–¹æ³•æ˜¯ä¸ºäº†ä¼˜é›…ç»ˆæ­¢ï¼šå°†é˜Ÿåˆ—æ¸…ç©ºï¼Œé˜²æ­¢èµ„æºæ³„æ¼æˆ–é˜»å¡

------

#### âœ… æ€»ç»“

è¿™æ®µä»£ç æ˜¯ä¸ºäº†**åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼‚æ­¥ä¼ è¾“æ•°æ®**ï¼Œè®©æ•°æ®å‡†å¤‡å’Œæ¨¡å‹è®­ç»ƒè§£è€¦ï¼Œæå‡å¹¶å‘æ€§èƒ½ã€‚å…¶ä¸­ `SimpleDataQueue` æ˜¯ä¸€ä¸ªè½»é‡çº§çš„çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ï¼Œç”¨äºåœ¨ `prepare_dataset()` å’Œ `update_actor()` ä¹‹é—´ä¼ é€’è®­ç»ƒæ ·æœ¬ã€‚







### 4. `tunix/rl/rollout/vanilla_rollout.py`

è¿™æ®µä»£ç å®ç°äº† Tunix ä¸­çš„ **Vanilla Rollout Worker**ï¼Œç”¨äº**ä» actor æ¨¡å‹ç”Ÿæˆæ–‡æœ¬**ï¼ˆä¹Ÿå°±æ˜¯æ¨ç† / rolloutï¼‰å¹¶æ”¯æŒ KV Cache ç®¡ç†å’Œ token logp çš„è®¡ç®—ã€‚æ˜¯æ•´ä¸ª GRPO / PPO å¼ºåŒ–å­¦ä¹ ä¸­çš„**æ ¸å¿ƒç”Ÿæˆæ¨¡å—**ã€‚

------

#### ğŸ§  ç±»èŒè´£æ¦‚è§ˆ

| ç±» / æ–¹æ³•               | åŠŸèƒ½                                       |
| ----------------------- | ------------------------------------------ |
| `VanillaRollout`        | æ‰˜ç®¡ rollout è¿‡ç¨‹ï¼Œä½¿ç”¨ `Sampler` å®Œæˆç”Ÿæˆ |
| `generate()`            | ä»æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼ˆæ ¸å¿ƒçš„ rollout æ­¥éª¤ï¼‰      |
| `get_per_token_logps()` | è·å–ç”Ÿæˆ token çš„é€ token log æ¦‚ç‡         |
| `update_params()`       | åœ¨ RL è®­ç»ƒä¸­æ¥æ”¶æ–°å‚æ•°å¹¶æ›´æ–°æ¨¡å‹           |
| `pad_id()` / `eos_id()` | è·å– tokenizer ä¸­çš„ç‰¹æ®Š token ID           |

------

#### ğŸ” å…³é”®ç»„ä»¶é€ä¸€è§£é‡Š

##### 1. `CacheConfig`

```python
@dataclasses.dataclass(frozen=True)
class CacheConfig:
  cache_size: int
  num_layers: int
  num_kv_heads: int
  head_dim: int
```

å®šä¹‰ KV Cache çš„é…ç½®å‚æ•°ï¼Œç”¨äºåˆå§‹åŒ– `Sampler` æ—¶çš„ç¼“å­˜ç©ºé—´å¤§å°ã€‚

------

##### 2. `VanillaRollout.__init__`

```python
self._sampler = sampler.Sampler(...)
```

- å®ä¾‹åŒ– `Sampler`ï¼Œç”¨äºæ‰§è¡Œé«˜æ•ˆçš„ token ç”Ÿæˆ
- ä¼ å…¥æ¨¡å‹ã€tokenizer å’Œ KV cache é…ç½®

------

##### 3. `generate(prompts, rollout_config, **kwargs)`

```python
output = self._sampler(
    input_strings=prompts,
    ...
)
return base_rollout.RolloutOutput(...)
```

- æ ¸å¿ƒæ¨ç†å‡½æ•°
- è°ƒç”¨ `Sampler.__call__` æ‰§è¡Œæ‰¹é‡æ–‡æœ¬ç”Ÿæˆ
- `RolloutConfig` æ§åˆ¶ç”Ÿæˆé•¿åº¦ã€æ¸©åº¦ã€top-pã€top-k ç­‰ sampling å‚æ•°
- è¾“å‡ºåŒ…æ‹¬æ–‡æœ¬ã€logitsã€tokens ä»¥åŠ padded çš„ prompt tokens

------

##### 4. `get_per_token_logps(prompt_tokens, completion_tokens)`

```python
return common.compute_per_token_logps(...)
```

- ç”¨äºè®¡ç®—ç­–ç•¥åœ¨ç”Ÿæˆ completion ä¸Šçš„ log æ¦‚ç‡
- ç”¨äºè®­ç»ƒæ—¶ reward åŠ æƒæˆ– KL æ•£åº¦è®¡ç®—

------

##### 5. `update_params(params)`

```python
flat_new_params = utils.to_flat_dict(params)
...
merged_params = jax.tree.unflatten(...)
self._sampler.transformer_state = ...
```

- ç”¨äº RL è®­ç»ƒä¸­å°†æ–°æƒé‡å‚æ•°æ›´æ–°åˆ° sampler çš„æ¨¡å‹ä¸­
- æ”¯æŒåŸåœ°æ›´æ–° transformer æƒé‡

------

##### 6. å…¶ä»–è¾…åŠ©æ–¹æ³•

```python
def pad_id(self) -> int:
def eos_id(self) -> int:
def model(self) -> nnx.Module:
```

- `pad_id()` å’Œ `eos_id()` è¿”å› tokenizer çš„ç‰¹æ®Š token ç¼–å·
- `model()` è¿”å›å½“å‰çš„ transformer æ¨¡å—å®ä¾‹

------

#### âœ… å°ç»“

ä½ å¯ä»¥ç†è§£ `VanillaRollout` æ˜¯ **actor æ¨¡å‹çš„ç”Ÿæˆæ¥å£åŒ…è£…å™¨**ï¼Œå®ƒï¼š

- ä½¿ç”¨ `Sampler` ç”Ÿæˆæ–‡æœ¬
- æ”¯æŒä»å¤–éƒ¨åŠ è½½å‚æ•°ï¼ˆç”¨äº policy æ›´æ–°ï¼‰
- æä¾› token-level logpï¼Œç”¨äº loss è®¡ç®—
- æ”¯æŒ GRPO/PPO ä¸­ rollout çš„æ‰€æœ‰éœ€æ±‚





### 5. `tunix/rl/common.py`

è¿™æ®µä»£ç æ˜¯ Tunix ä¸­ RL è®­ç»ƒæ‰€ç”¨çš„**é€šç”¨è¾…åŠ©å‡½æ•°é›†åˆ**ï¼Œç”¨äºå¤„ç†ï¼š

1. æ¨¡å‹ log probability çš„è®¡ç®—
2. mask å’Œ attention mask æ„é€ 
3. padding ä¸ä½ç½®ç¼–ç 
4. ä¸º GRPO / PPO ä¸­ KL loss å’Œ advantage è®¡ç®—ç­‰æä¾›åº•å±‚æ”¯æŒ

------

#### ğŸ” å…³é”®å‡½æ•°è§£é‡Š

#### âœ… 1. `selective_log_softmax(logits, input_ids)`

ä»æ¨¡å‹ logits ä¸­æå–æ¯ä¸ª token çš„ log probabilityï¼š

```python
logps = jax.nn.log_softmax(logits, axis=-1)
per_token_logps = jnp.take_along_axis(logps, input_ids[..., None], axis=-1)
return per_token_logps[..., 0]
```

------

#### âœ… 2. `get_per_token_logps(model, input_tokens, positions, attn_mask, logits_to_keep)`

æ‰§è¡Œæ¨¡å‹å‰å‘ï¼Œå¾—åˆ° logitsï¼Œé€‰å–æœ€å `logits_to_keep` ä¸ª tokenï¼š

```python
logits, _ = model(...)
logits = logits[:, -logits_to_keep - 1 : -1, :]
input_tokens = input_tokens[:, -logits_to_keep:]
return selective_log_softmax(logits, input_tokens)
```

ç”¨äº rewardã€KL æˆ– RL loss çš„ token logp è®¡ç®—ã€‚

------

#### âœ… 3. `compute_per_token_logps(model, prompt_tokens, completion_tokens, pad_id, eos_id)`

å®Œæ•´åŒ…è£…å‡½æ•°ï¼Œæ¥æ”¶ `prompt + completion`ï¼š

- æ‹¼æ¥åæ„é€  `prompt_completion_mask`
- ç”Ÿæˆ RoPE æ‰€éœ€çš„ä½ç½®ç¼–ç  + causal attention mask
- è°ƒç”¨ `get_per_token_logps`

å¯é€‰ `stop_gradient` ä»¥é¿å…åå‘ä¼ æ’­ã€‚

------

#### âœ… 4. `make_completion_mask(completion_ids, eos_tok)`

ä¸ºæ¯æ¡ `completion` ç”Ÿæˆ maskï¼Œé‡åˆ° `eos_id` å paddingï¼š

```python
completion_mask = (sequence_indices <= eos_idx[:, None]).astype(jnp.int32)
```

ä¾‹å¦‚ï¼š
 è¾“å…¥ï¼š`[42, 17, 9, <eos>, 0, 0]`
 è¾“å‡ºï¼š`[1, 1, 1, 1, 0, 0]`

------

#### âœ… 5. `make_causal_attn_mask(input_mask)`

æ„é€  causal attention maskï¼ˆæ¯ä¸ª token åªèƒ½çœ‹è‡ªå·±å’Œå‰é¢ï¼‰ï¼š

```python
causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))
attn_mask = input_mask[..., None, :] * causal_mask[None, ...]
```

------

#### âœ… 6. `pad_to_length(x, target_length, pad_value, left=False, axis=0)`

é€šç”¨çš„ padding å·¥å…·å‡½æ•°ï¼Œå¯ä»¥æŒ‡å®šï¼š

- å·¦å³ pad
- å“ªä¸ªè½´ pad

ä¾‹å¦‚ï¼š

```python
pad_to_length([1, 2], 4, pad_value=0) â†’ [1, 2, 0, 0]
```

------

#### âœ… 7. `build_positions_from_mask(input_mask)`

æ ¹æ® mask æ„å»ºä½ç½®ç¼–ç ç´¢å¼•ï¼š

```python
positions = jnp.cumsum(input_mask, axis=-1) - 1
```

- å¦‚æœ mask ä¸º `[1, 1, 0, 1]`
   â†’ ä½ç½®ä¸º `[0, 1, 0, 2]` ï¼ˆè·³è¿‡ padï¼‰










### 6. `tunix/rl/reshard.py`

è¿™æ®µä»£ç æä¾›äº† **JAX ä¸­ç”¨äºè·¨ mesh æˆ–è·¨ sharding çš„å‚æ•°è¿ç§»ï¼ˆreshardingï¼‰åŠŸèƒ½**ï¼Œæ˜¯ Tunix æ¡†æ¶ä¸­æ”¯æŒå¤šè®¾å¤‡è®­ç»ƒï¼ˆå°¤å…¶æ˜¯ TPUsï¼‰æ—¶çš„é‡è¦ç»„ä»¶ã€‚

------

#### ğŸ”§ åŠŸèƒ½ç®€è¿°

- `reshard_pytree(...)`ï¼šå°†ä¸€ä¸ª `PyTree`ï¼ˆé€šå¸¸æ˜¯æ¨¡å‹å‚æ•°ï¼‰ä»ä¸€ç§è®¾å¤‡åˆ†å¸ƒæ–¹å¼ï¼ˆshardingï¼‰è¿ç§»åˆ°å¦ä¸€ç§
- æ”¯æŒä½¿ç”¨ Google å†…éƒ¨çš„ Pathways å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™é™çº§ä¸º `jax.device_put`
- æä¾›å¼‚æ­¥ ready å›è°ƒ `callback_on_ready`ï¼Œç”¨äºè®°å½•è¿ç§»æˆåŠŸæˆ–å¤±è´¥çš„è€—æ—¶

------

#### ğŸ“¦ é€æ­¥è®²è§£

#### 1. `callback_on_ready`

```python
def callback_on_ready(x, success, failure):
```

- ç”¨äºåœ¨ resharding ç»“æŸåï¼Œ**å¼‚æ­¥å›è°ƒ success æˆ– failure**
- ä¼šå¼€ä¸€ä¸ªæ–°çº¿ç¨‹æ‰§è¡Œ `jax.block_until_ready(x)` æ¥é˜»å¡ç­‰å¾…æ•°ç»„å®Œæˆï¼Œç„¶åè§¦å‘ callback

------

#### 2. `reshard_pytree(...)`

æ ¸å¿ƒå‡½æ•°ï¼Œå‚æ•°å¦‚ä¸‹ï¼š

```python
reshard_pytree(source, target, cache_plan=True, donate_input=False, use_experimental_pre_reshard=True)
```

- `source`ï¼šåŸå§‹ tensor æˆ– PyTreeï¼ˆåŒ…å«åˆ†å¸ƒç­–ç•¥ï¼‰
- `target`ï¼šç›®æ ‡ tensor æˆ– PyTreeï¼Œç”¨æ¥æä¾›ç›®æ ‡ sharding ä¿¡æ¯
- `cache_plan`ï¼šæ˜¯å¦ç¼“å­˜ reshard è®¡åˆ’ï¼ŒåŠ é€Ÿåç»­é‡å¤è¿ç§»
- `donate_input`ï¼šæ˜¯å¦æŠŠè¾“å…¥æ•°ç»„æ‰€æœ‰æƒâ€œè®©æ¸¡â€å‡ºå»ï¼ˆå‡å°‘æ‹·è´ï¼‰
- `use_experimental_pre_reshard`ï¼šæ˜¯å¦å°è¯•ä½¿ç”¨è°·æ­Œå†…éƒ¨ä¼˜åŒ–è·¯å¾„

------

#### 3. `_get_dst_sharding`

ç”¨äºä» `target` æ ‘ä¸­æå–æˆ–æ„å»ºç›®æ ‡ `sharding` å¯¹è±¡ï¼š

```python
if isinstance(x, NamedSharding | SingleDeviceSharding): return x
else: return NamedSharding(x.sharding.mesh, x.sharding.spec)
```

è¿™ä¸ªå‡½æ•°ç”¨äºå¤„ç† JAX çš„ Sharding å¯¹è±¡ã€‚

------

#### 4. åŠ¨æ€é€‰æ‹© reshard å®ç°

```python
if reshardfn is None:
  try:
    import pathwaysutils
    from pathwaysutils.experimental import reshard
    reshardfn = functools.partial(experimental_reshard.reshard, x=source)
  except ImportError:
    logging.error("Can't import PathwaysUtils...")
```

- å¦‚æœèƒ½ç”¨ Google å†…éƒ¨çš„ `pathwaysutils.experimental.reshard`ï¼Œå°±ç”¨å®ƒï¼ˆå¯èƒ½æ›´å¿«ï¼‰
- å¦åˆ™ fallback åˆ°ï¼š

```python
jax.device_put(source, dst_shardings)
```

è¿™æ˜¯æ™®é€šçš„è®¾å¤‡è¿ç§»å‡½æ•°ã€‚

------

#### 5. å¼‚æ­¥æ—¥å¿—è®°å½•

```python
callback_on_ready(resharded_array, on_success, on_failure)
```

è¿ç§»å®Œæˆåè®°å½•æ—¥å¿—ï¼ŒåŒ…æ‹¬ç”¨æ—¶æˆ–å¤±è´¥ä¿¡æ¯ã€‚







### 7. `tunix/rl/rl_cluster.py`

è¿™æ®µä»£ç å®šä¹‰äº† `RLCluster` ç±»ï¼Œæ˜¯ Tunix æ¡†æ¶ä¸­é¢å‘ç”¨æˆ·çš„**æ ¸å¿ƒæ¥å£ç±»**ï¼Œç”¨äºç®¡ç†å’Œåè°ƒ RLHF / GRPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­æ¶‰åŠçš„å¤šä¸ªæ¨¡å‹ä¸å­ç»„ä»¶ã€‚

------

#### ğŸ§  ç®€æ´ç†è§£ï¼šRLCluster åšä»€ä¹ˆï¼Ÿ

`RLCluster` å°è£…äº†ï¼š

| æ¨¡å—               | åŠŸèƒ½                                             |
| ------------------ | ------------------------------------------------ |
| `train_actor`      | RL è®­ç»ƒä½¿ç”¨çš„ actor æ¨¡å‹                         |
| `rollout_actor`    | rollout ç”¨äºç”Ÿæˆ response çš„ actor æ¨¡å‹          |
| `critic`           | å€¼å‡½æ•°æ¨¡å‹                                       |
| `reference`        | KL æ•£åº¦å‚è€ƒæ¨¡å‹                                  |
| `reward`           | å¥–åŠ±æ¨¡å‹                                         |
| `trainer`          | ç”¨äºè®­ç»ƒæ¨¡å‹çš„ä¼˜åŒ–å™¨å’Œè®­ç»ƒé€»è¾‘                   |
| `inference_worker` | åŒ…è£… reference / reward / critic ç”¨äºè¯„ä¼°        |
| `rollout`          | è°ƒç”¨ `Sampler` å®é™…ç”Ÿæˆæ–‡æœ¬çš„æ¥å£                |
| `sync_weights()`   | è®­ç»ƒå’Œé‡‡æ ·æ¨¡å‹ä¹‹é—´åŒæ­¥å‚æ•°ï¼ˆæ”¯æŒ LoRA å’Œå…¨å‚æ•°ï¼‰ |

------

#### ğŸ” æ ¸å¿ƒé€»è¾‘æ‹†è§£

#### 1. `__init__`

æ„é€ å‡½æ•°ä¸­ä¸»è¦åšäº† 3 ä»¶äº‹ï¼š

- åŠ è½½æ‰€æœ‰æ¨¡å‹åˆ°æŒ‡å®š `Mesh` ä¸Šï¼ˆå¯èƒ½ä¼šè‡ªåŠ¨ reshardï¼‰
- æ„é€  rollout æ¨¡å‹ï¼ˆä½¿ç”¨ `VanillaRollout` æˆ–å°†æ¥æ”¯æŒ `vLLM`ï¼‰
- æ„é€  inference_worker + trainer

```python
self.train_actor = self._load_model(actor, mesh)
self._rollout = VanillaRollout(...)
self._inference_worker = InferenceWorker(...)
self._actor_trainer = Trainer(...)
```

------

#### 2. `_load_model(...)`

```python
def _load_model(self, model_or_path: nnx.Module | str, mesh: Mesh)
```

- å¦‚æœæ˜¯ `nnx.Module` å®ä¾‹ï¼Œä¼šæ£€æŸ¥æ¨¡å‹å½“å‰ mesh æ˜¯å¦åŒ¹é…ç›®æ ‡ mesh
- å¦‚æœä¸åŒ¹é…ï¼Œåˆ™è°ƒç”¨ `reshard.reshard_pytree()` è¿ç§»æƒé‡
- æš‚ä¸æ”¯æŒä»è·¯å¾„åŠ è½½æ¨¡å‹ï¼ˆ`NotImplementedError`ï¼‰

------

#### 3. `generate(prompts)`

```python
return self.rollout.generate(prompts, rollout_config)
```

åœ¨ `ROLLOUT` è®¾å¤‡ä¸Šæ‰§è¡Œï¼Œè°ƒç”¨ `Sampler` ç”Ÿæˆ responseã€‚

------

#### 4. `update_actor()` / `update_critic()`

```python
self.actor_trainer.train(train_ds, eval_ds, skip_jit)
```

è°ƒç”¨ `Trainer.train()` æ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°ã€‚

------

#### 5. `get_ref_per_token_logps()` / `get_old_per_token_logps()`

åˆ†åˆ«ç”¨äºï¼š

- KL æ•£åº¦è®¡ç®—ï¼ˆç”± reference æ¨¡å‹æ¨ç†ï¼‰
- æ—§ç­–ç•¥ logp è·å–ï¼ˆç”± rollout actor æ¨ç†ï¼‰

------

#### 6. `sync_weights()`

```python
if is_lora_enabled(model):
    src = LoRAParam(actor_trainer)
    dst = LoRAParam(rollout)
else:
    src = Param(actor_trainer)
    dst = Param(rollout)
reshard_pytree(src, dst)
rollout.update_params(...)
```

- å°†è®­ç»ƒå¾—åˆ°çš„æ–°æƒé‡åŒæ­¥åˆ° rollout actorï¼Œç”¨äºä¸‹ä¸€è½®é‡‡æ ·
- æ”¯æŒ LoRA ä¸å…¨å‚æ•°ä¸¤ç§åŒæ­¥æ–¹å¼
- ä½¿ç”¨ `reshard_pytree` ä¿éšœ mesh / sharding å¯¹é½

------

#### âœ… æ€»ç»“

å¯ä»¥æŠŠ `RLCluster` ç†è§£ä¸º Tunix ä¸­ RLHF çš„â€œè®­ç»ƒåè°ƒä¸­å¿ƒâ€ï¼Œå®ƒå±è”½äº†å¤šè®¾å¤‡éƒ¨ç½²ã€æ¨¡å‹è¿ç§»ã€é‡‡æ ·ã€è®­ç»ƒã€è¯„ä¼°ç­‰æ‰€æœ‰å¤æ‚ç»†èŠ‚ï¼Œç»Ÿä¸€å¯¹å¤–æä¾›ï¼š

- `generate()` â†’ rollout
- `update_actor()` / `update_critic()` â†’ è®­ç»ƒ
- `sync_weights()` â†’ rollout <-> trainer æƒé‡åŒæ­¥





### 8. `tunix/rl/trainer.py`

è¿™æ®µä»£ç å®šä¹‰äº† `Trainer` ç±»ï¼Œæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­å¯¹ `PeftTrainer` çš„ä¸€ä¸ªæ‰©å±•ç‰ˆæœ¬ï¼Œæ ¸å¿ƒç›®çš„æ˜¯ä¸º **RL è®­ç»ƒè¿‡ç¨‹æ·»åŠ æ—¥å¿—è®°å½•å’Œè¿›åº¦æ¡æ˜¾ç¤ºåŠŸèƒ½**ã€‚

å®ƒæ˜¯æ•´ä¸ª Tunix RLHF / GRPO æ¡†æ¶ä¸­ç”¨äºè®­ç»ƒ actor / critic çš„é€šç”¨è®­ç»ƒå™¨ã€‚

------

#### ğŸ§  ç±»ç»“æ„è¯´æ˜ï¼š`Trainer`

ç»§æ‰¿è‡ªï¼š

```python
from tunix.sft import peft_trainer
class Trainer(peft_trainer.PeftTrainer)
```

ä¸»è¦èŒè´£æ˜¯ï¼š

1. æ·»åŠ å’Œè®°å½• RL ä¸­è‡ªå®šä¹‰æŒ‡æ ‡ï¼ˆå¦‚ KLï¼‰
2. æ˜¾ç¤ºè‡ªå®šä¹‰è¿›åº¦æ¡æŒ‡æ ‡ï¼ˆæ¯”å¦‚ reward, kl ç­‰ï¼‰
3. ä¿ç•™åŸå§‹ SFT è®­ç»ƒçš„èƒ½åŠ›ï¼ˆæ¥è‡ª PeftTrainerï¼‰

------

#### ğŸ” æ–¹æ³•é€é¡¹è§£é‡Š

#### `__init__(...)`

è°ƒç”¨ `super()` åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œè®­ç»ƒé…ç½®ã€‚ç„¶ååˆå§‹åŒ–ä¸¤ä¸ªå­—æ®µï¼š

```python
self.rl_metrics_to_log = {}  # eg: {"kl": "kl"}
self.tqdm_metrics_to_display = []  # eg: ["kl", lambda: "rewards/overall"]
```

------

#### `with_rl_metrics_to_log(...)`

```python
def with_rl_metrics_to_log(self, rl_metrics_to_log: dict[str, str])
```

æ³¨å†Œä½ å¸Œæœ›è®°å½•åˆ° `metrics_logger` ä¸­çš„æŒ‡æ ‡ã€‚

ç¤ºä¾‹ï¼š

```python
trainer.with_rl_metrics_to_log({"kl": "kl"})
```

è¡¨ç¤ºä» `train_step` çš„ `aux["kl"]` ä¸­å–å‡ºå€¼å¹¶è®°å½•ä¸º `"kl"`ã€‚

------

#### `with_tqdm_metrics_to_display(...)`

```python
def with_tqdm_metrics_to_display(self, tqdm_metrics_to_display: list[str | Callable[[], str]])
```

æ§åˆ¶å“ªäº›æŒ‡æ ‡ä¼šæ˜¾ç¤ºåœ¨è®­ç»ƒè¿›åº¦æ¡ä¸Šã€‚å¯ä»¥æ˜¯é™æ€å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯å‡½æ•°è¿”å›çš„ keyã€‚

------

#### `_post_process_train_step` / `_post_process_eval_step`

```python
self.metrics_logger.log(metric_name, aux[metric_key], mode, steps)
```

- è¢« `train_step` å’Œ `eval_step` è°ƒç”¨åæ‰§è¡Œ
- ä¼šæŠŠ `aux` ä¸­çš„å€¼è®°å½•ä¸‹æ¥ï¼ˆå¦‚ KL æ•£åº¦ã€å¥–åŠ±ç­‰ï¼‰
- ç”¨äº `wandb` ç­‰ç³»ç»Ÿç»Ÿä¸€æ—¥å¿—è®°å½•

------

#### `_get_additional_tqdm_metrics`

- è¿”å› tqdm ä¸­è¦æ˜¾ç¤ºçš„é¢å¤–æŒ‡æ ‡åˆ—è¡¨ï¼ˆé˜²æ­¢é‡å¤ï¼‰
- æ”¯æŒåŠ¨æ€å’Œé™æ€ key æ··ç”¨

------

#### `_tqdm_train_metrics` / `_tqdm_eval_metrics`

é‡å†™çˆ¶ç±»å±æ€§ï¼Œè¿½åŠ  RL ä¸­è®¾ç½®çš„æ˜¾ç¤ºé¡¹ã€‚

------

#### âœ… æ€»ç»“ä½œç”¨ï¼ˆç®€æ´ï¼‰

è¿™ä¸ª `Trainer` æ˜¯ `PeftTrainer` çš„ RL åŠ å¼ºç‰ˆï¼Œä¸“é—¨ä¸º **GRPO/PPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹**å¢åŠ ï¼š

- è‡ªå®šä¹‰ metric è®°å½•ï¼ˆå¦‚ KL/rewardï¼‰
- è‡ªå®šä¹‰è¿›åº¦æ¡è¾“å‡º
- ä¿ç•™ optax ä¼˜åŒ–å™¨è®­ç»ƒæµç¨‹

æ•´ä¸ª Tunix ä¸­ï¼Œ`GrpoLearner` å°±æ˜¯é€šè¿‡ï¼š

```python
rl_cluster.actor_trainer.with_loss_fn(...)
rl_cluster.actor_trainer.with_rl_metrics_to_log(...)
```

æŠŠ loss å‡½æ•°ã€æŒ‡æ ‡æ¥å…¥è¿›æ¥çš„ã€‚

æ˜¯å¦éœ€è¦æˆ‘è®²å®ƒå¦‚ä½•è°ƒç”¨ `train_step()`ã€æˆ–ç»“åˆå®Œæ•´è®­ç»ƒ loop å±•ç¤ºè°ƒç”¨è·¯å¾„ï¼Ÿ









### 9. `tunix/rl/utils.py`

è¿™æ®µä»£ç æ˜¯ GRPOï¼ˆGroup Relative Policy Optimizationï¼‰ä¸­çš„è¾…åŠ©å·¥å…·ï¼Œç”¨äºå¤„ç†ï¼š

1. **å‚æ•°ç»“æ„çš„æ‰å¹³åŒ–**ï¼ˆ`to_flat_dict`ï¼‰
2. **æ¨¡å‹å‚æ•°çš„ mesh åˆ†å¸ƒä¿¡æ¯æå–**ï¼ˆ`get_pytree_mesh_info`ï¼‰

å®ƒä¸»è¦ç”¨äº reshardã€æ¨¡å‹åŒæ­¥ã€å‚æ•°æ£€æŸ¥ç­‰æ­¥éª¤ï¼Œç¡®ä¿åœ¨å¤šè®¾å¤‡è®­ç»ƒï¼ˆTPU/GPUï¼‰ä¸­å‚æ•°çš„ä¸€è‡´æ€§ã€‚

------

#### ğŸ“¦ å‡½æ•°è§£é‡Š

------

#### âœ… `to_flat_dict(tree: PyTree)`

```python
def to_flat_dict(tree) -> tuple[dict[tuple[str, ...], Array], PyTreeDef]
```

**åŠŸèƒ½ï¼š**
 å°†ä¸€ä¸ªæ¨¡å‹çš„ `PyTree`ï¼ˆå¦‚ `nnx.state(model)` è¿”å›çš„çŠ¶æ€ï¼‰å±•å¼€æˆï¼š

- ä¸€ä¸ªæ‰å¹³çš„ `dict`ï¼Œkey æ˜¯è·¯å¾„å…ƒç»„ï¼ˆå¦‚ `("layer1", "dense", "weight")`ï¼‰
- ä¸€ä¸ª `PyTreeDef`ï¼Œç”¨äºåç»­é‡å»ºç»“æ„

**ç”¨é€”ï¼š**
 åœ¨ `update_params()` ä¸­ç”¨äºåˆå¹¶æ–°æ—§å‚æ•°ï¼š

```python
flat_new_params, _ = to_flat_dict(params)
flat_old_params, tree_def = to_flat_dict(self._sampler.transformer_state)
```

------

#### âœ… `get_pytree_mesh_info(tree: PyTree) -> Mesh | None`

**åŠŸèƒ½ï¼š**
 éå†ä¸€ä¸ª `PyTree`ï¼Œæå–æ‰€æœ‰ `jax.Array` çš„ `.sharding.mesh` ä¿¡æ¯

```python
if isinstance(sharding, NamedSharding):
    mesh_info.add(sharding.mesh)
```

å¦‚æœå‘ç°å¤šç§ meshï¼Œä¼šæŠ¥é”™ï¼›å¦åˆ™è¿”å›å”¯ä¸€ meshã€‚

**ç”¨é€”ï¼š**
 åœ¨ `RLCluster._load_model()` ä¸­ç”¨äºåˆ¤æ–­æ¨¡å‹æ˜¯å¦å·²ç»ä½äºç›®æ ‡ mesh ä¸Šï¼š

```python
model_mesh = get_pytree_mesh_info(nnx.state(model))
if model_mesh != mesh:
    reshard(...)
```

------

#### âœ… æ€»ç»“

| å‡½æ•°                   | ç”¨é€”                                             |
| ---------------------- | ------------------------------------------------ |
| `to_flat_dict`         | æŠŠæ¨¡å‹å‚æ•°å±•å¹³ä¸º dictï¼Œæ–¹ä¾¿ä¿®æ”¹æˆ–åˆå¹¶            |
| `get_pytree_mesh_info` | è·å–å‚æ•°åœ¨å“ªä¸ª mesh ä¸Šï¼Œè¾…åŠ©åˆ¤æ–­æ˜¯å¦éœ€è¦ reshard |

è¿™äº›å‡½æ•°åœ¨ Tunix çš„æƒé‡åŒæ­¥ã€å‚æ•°è¿ç§»ã€æ¨¡å‹åŠ è½½ä¸­éå¸¸å…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè®¾å¤‡å¹¶è¡Œåœºæ™¯ä¸‹ã€‚
